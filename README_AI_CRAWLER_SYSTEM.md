# ğŸ¤– Sistema de Rastreo IA - evolve2digital.com

## ğŸ“‹ Resumen

Sistema completo de configuraciÃ³n, monitoreo y seguridad para crawlers de IA (GPTBot, Google-Extended, ClaudeBot, etc.) implementado en Next.js con TypeScript.

### âœ¨ CaracterÃ­sticas Principales

- ğŸ¯ **Acceso EstratÃ©gico**: Permite acceso controlado a crawlers IA legÃ­timos
- ğŸ“Š **Monitoreo Completo**: Logging detallado y mÃ©tricas en tiempo real
- ğŸ”’ **Seguridad Robusta**: Rate limiting, detecciÃ³n de anomalÃ­as y protecciÃ³n contra abusos
- ğŸ¥ **Health Checks**: VerificaciÃ³n automÃ¡tica de accesibilidad
- ğŸš¨ **Alertas**: Sistema de notificaciones para incidentes
- ğŸ“ˆ **APIs de GestiÃ³n**: Endpoints para administraciÃ³n y configuraciÃ³n

---

## ğŸš€ Inicio RÃ¡pido

### Verificar Estado del Sistema

```bash
# Comprobar que todo funciona
node scripts/monitor-ai-crawlers.js stats

# Probar acceso de GPTBot
curl -H "User-Agent: GPTBot/1.0" https://evolve2digital.com/es

# Verificar logs
tail -f logs/ai-crawlers/$(date +%Y-%m-%d).log
```

### Comandos Esenciales

```bash
# Monitoreo automÃ¡tico
node scripts/monitor-ai-crawlers.js monitor

# EstadÃ­sticas de seguridad
curl -X GET "http://localhost:3000/api/admin/ai-crawler-security?action=stats"

# Health check especÃ­fico
curl -X GET "http://localhost:3000/api/admin/ai-crawler-monitor?action=health-check&crawler=GPTBot"
```

---

## ğŸ—ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   robots.txt    â”‚    â”‚   Middleware     â”‚    â”‚   AI Logger     â”‚
â”‚   (Permisos)    â”‚â”€â”€â”€â–¶â”‚   (Seguridad)    â”‚â”€â”€â”€â–¶â”‚   (Registro)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Monitor API   â”‚    â”‚   Security API   â”‚    â”‚   Health Check  â”‚
â”‚   (EstadÃ­sticas)â”‚    â”‚   (ConfiguraciÃ³n)â”‚    â”‚   (VerificaciÃ³n)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Componentes Clave

| Componente | Archivo | DescripciÃ³n |
|------------|---------|-------------|
| **Robots Config** | `app/robots.ts` | ConfiguraciÃ³n de permisos para crawlers |
| **Middleware** | `middleware.ts` | Interceptor de requests con seguridad |
| **AI Logger** | `lib/ai-crawler-logger.ts` | Sistema de logging especializado |
| **Monitor** | `lib/ai-crawler-monitor.ts` | Health checks y alertas |
| **Security** | `lib/ai-crawler-security.ts` | Rate limiting y protecciÃ³n |
| **Monitor Script** | `scripts/monitor-ai-crawlers.js` | Herramienta de monitoreo CLI |

---

## ğŸ”§ ConfiguraciÃ³n

### Crawlers Soportados

| Crawler | User-Agent | Rate Limit | Estado |
|---------|------------|------------|--------|
| **GPTBot** | `GPTBot/1.0` | 30 req/min | âœ… Activo |
| **Google-Extended** | `Google-Extended/1.0` | 60 req/min | âœ… Activo |
| **ClaudeBot** | `ClaudeBot/1.0` | 30 req/min | âœ… Activo |
| **ChatGPT-User** | `ChatGPT-User/1.0` | 20 req/min | âœ… Activo |
| **Bingbot** | `Bingbot/2.0` | 40 req/min | âœ… Activo |

### Rutas Permitidas

```typescript
const allowedPaths = [
  '/',              // PÃ¡gina principal
  '/es/',           // EspaÃ±ol
  '/en/',           // InglÃ©s
  '/es/blog/',      // Blog espaÃ±ol
  '/en/blog/',      // Blog inglÃ©s
  '/es/docs/',      // DocumentaciÃ³n espaÃ±ol
  '/en/docs/',      // DocumentaciÃ³n inglÃ©s
  '/sitemap.xml',   // Sitemap
  '/rss.xml'        // RSS Feed
]
```

### Rutas Bloqueadas

```typescript
const disallowedPaths = [
  '/api/',          // APIs internas
  '/admin/',        // Panel de administraciÃ³n
  '/_next/',        // Archivos de Next.js
  '/private/',      // Contenido privado
  '/*.json$'        // Archivos JSON
]
```

---

## ğŸ“Š APIs de AdministraciÃ³n

### Monitor API

```bash
# EstadÃ­sticas generales
GET /api/admin/ai-crawler-monitor?action=stats

# Health check especÃ­fico
GET /api/admin/ai-crawler-monitor?action=health-check&crawler=GPTBot&url=https://evolve2digital.com/es

# ConfiguraciÃ³n actual
GET /api/admin/ai-crawler-monitor?action=config

# Ejecutar monitoreo completo
POST /api/admin/ai-crawler-monitor?action=run-cycle

# Probar crawler especÃ­fico
POST /api/admin/ai-crawler-monitor?action=test-crawler
```

### Security API

```bash
# EstadÃ­sticas de seguridad
GET /api/admin/ai-crawler-security?action=stats

# ConfiguraciÃ³n de seguridad
GET /api/admin/ai-crawler-security?action=config

# Activar modo emergencia
POST /api/admin/ai-crawler-security?action=emergency-mode

# Gestionar blacklist
POST /api/admin/ai-crawler-security?action=blacklist

# Actualizar rate limits
POST /api/admin/ai-crawler-security?action=rate-limits

# Limpiar datos antiguos
DELETE /api/admin/ai-crawler-security?action=cleanup
```

---

## ğŸ› ï¸ InstalaciÃ³n y Setup

### Requisitos

- Node.js 18+
- Next.js 14+
- TypeScript
- Permisos de escritura en `/logs/`

### InstalaciÃ³n

```bash
# 1. Los archivos ya estÃ¡n incluidos en el proyecto
# 2. Crear directorio de logs
mkdir -p logs/ai-crawlers

# 3. Hacer ejecutable el script de monitoreo
chmod +x scripts/monitor-ai-crawlers.js

# 4. Probar configuraciÃ³n
npm run dev
node scripts/monitor-ai-crawlers.js test
```

### ConfiguraciÃ³n de ProducciÃ³n

```bash
# 1. Configurar cron job para monitoreo automÃ¡tico
crontab -e
# Agregar: 0,30 * * * * /usr/bin/node /path/to/monitor-ai-crawlers.js monitor

# 2. Configurar rotaciÃ³n de logs (opcional)
# Agregar a logrotate.d/ai-crawlers:
# /path/to/logs/ai-crawlers/*.log {
#   daily
#   rotate 30
#   compress
#   delaycompress
#   missingok
#   notifempty
# }

# 3. Verificar permisos
chown -R www-data:www-data logs/ai-crawlers/
chmod 755 logs/ai-crawlers/
```

---

## ğŸ“ˆ Monitoreo y MÃ©tricas

### MÃ©tricas Clave

- **Success Rate**: >95% esperado
- **Response Time**: <2000ms promedio
- **Rate Limit Violations**: <10/dÃ­a
- **Security Violations**: 0 crÃ­ticas
- **Active Crawlers**: 3-5 simultÃ¡neos

### Comandos de Monitoreo

```bash
# Dashboard rÃ¡pido
node scripts/monitor-ai-crawlers.js stats

# Logs en tiempo real
tail -f logs/ai-crawlers/$(date +%Y-%m-%d).log

# EstadÃ­sticas de seguridad
curl -s -X GET "http://localhost:3000/api/admin/ai-crawler-security?action=stats" | jq

# Health check manual
curl -H "User-Agent: GPTBot/1.0" -v https://evolve2digital.com/es
```

### Alertas AutomÃ¡ticas

El sistema genera alertas cuando:
- Success rate <95%
- Response time >5000ms
- >10 errores por hora
- Violaciones de seguridad crÃ­ticas
- Health checks fallan consecutivamente

---

## ğŸš¨ Troubleshooting

### Problemas Comunes

#### 1. Crawler No Puede Acceder (403/404)

```bash
# Verificar robots.txt
curl https://evolve2digital.com/robots.txt

# Comprobar blacklist
curl -s -X GET "/api/admin/ai-crawler-security?action=config" | jq '.data.ipBlacklist'

# Probar acceso manual
curl -H "User-Agent: GPTBot/1.0" -v https://evolve2digital.com/es
```

#### 2. Rate Limiting Excesivo (429)

```bash
# Revisar lÃ­mites actuales
curl -s -X GET "/api/admin/ai-crawler-security?action=config" | jq '.data.crawlerRateLimits'

# Aumentar lÃ­mites temporalmente
curl -X POST "/api/admin/ai-crawler-security?action=rate-limits" \
  -H "Content-Type: application/json" \
  -d '{"crawlerType": "GPTBot", "limits": {"requestsPerMinute": 100}}'
```

#### 3. Logs No Se Generan

```bash
# Verificar permisos
ls -la logs/ai-crawlers/

# Comprobar middleware
grep "AI-CRAWLER" logs/application.log

# Probar logging manual
curl -H "User-Agent: GPTBot/1.0" https://evolve2digital.com/es
tail -1 logs/ai-crawlers/$(date +%Y-%m-%d).log
```

### DiagnÃ³stico RÃ¡pido

```bash
# Script de diagnÃ³stico completo
cat > scripts/diagnose.sh << 'EOF'
#!/bin/bash
echo "=== DIAGNÃ“STICO SISTEMA AI CRAWLER ==="
echo "1. Estado del servidor:"
curl -I https://evolve2digital.com/ 2>/dev/null | head -1

echo "2. Robots.txt:"
curl -s https://evolve2digital.com/robots.txt | grep -E "(GPTBot|Allow|Disallow)" | head -5

echo "3. Middleware activo:"
curl -H "User-Agent: GPTBot/1.0" -I https://evolve2digital.com/es 2>/dev/null | head -1

echo "4. Logs del dÃ­a:"
ls -la logs/ai-crawlers/$(date +%Y-%m-%d).log 2>/dev/null || echo "âŒ No hay logs hoy"

echo "5. API de monitoreo:"
curl -s -X GET "http://localhost:3000/api/admin/ai-crawler-monitor?action=stats" >/dev/null 2>&1 && echo "âœ… API responde" || echo "âŒ API no responde"

echo "=== FIN DIAGNÃ“STICO ==="
EOF

chmod +x scripts/diagnose.sh
./scripts/diagnose.sh
```

---

## ğŸ”’ Seguridad

### Medidas Implementadas

- **Rate Limiting**: LÃ­mites por crawler y por IP
- **Blacklist/Whitelist**: Control de IPs
- **DetecciÃ³n de AnomalÃ­as**: Patrones sospechosos
- **Modo Emergencia**: Bloqueo temporal en caso de ataques
- **Logging Seguro**: RotaciÃ³n automÃ¡tica y cumplimiento GDPR

### ConfiguraciÃ³n de Seguridad

```typescript
const securityConfig = {
  rateLimits: {
    'GPTBot': { requestsPerMinute: 30, burstLimit: 10 },
    'Google-Extended': { requestsPerMinute: 60, burstLimit: 15 }
  },
  anomalyPatterns: ['/admin', '/.env', '/wp-admin'],
  emergencyMode: { enabled: false, maxRequestsPerMinute: 5 }
}
```

### Respuesta a Incidentes

```bash
# Activar modo emergencia
curl -X POST "/api/admin/ai-crawler-security?action=emergency-mode" \
  -d '{"enabled": true, "maxRequestsPerMinute": 5}'

# Bloquear IP problemÃ¡tica
curl -X POST "/api/admin/ai-crawler-security?action=blacklist" \
  -d '{"ip": "MALICIOUS_IP", "operation": "add"}'

# Limpiar violaciones
curl -X DELETE "/api/admin/ai-crawler-security?action=cleanup"
```

---

## ğŸ“š DocumentaciÃ³n Adicional

### Archivos de DocumentaciÃ³n

- [`docs/AI_CRAWLER_CONFIGURATION.md`](docs/AI_CRAWLER_CONFIGURATION.md) - ConfiguraciÃ³n tÃ©cnica completa
- [`docs/OPERATIONAL_PROCEDURES.md`](docs/OPERATIONAL_PROCEDURES.md) - Procedimientos operativos diarios
- Este archivo - GuÃ­a de inicio rÃ¡pido

### Referencias Externas

- [OpenAI GPTBot Documentation](https://platform.openai.com/docs/gptbot)
- [Google Extended Crawler](https://developers.google.com/search/docs/crawling-indexing/overview-google-crawlers)
- [Anthropic Claude Bot](https://docs.anthropic.com/claude/docs)

### Estructura de Archivos

```
â”œâ”€â”€ app/
â”‚   â””â”€â”€ robots.ts                           # ConfiguraciÃ³n robots.txt
â”œâ”€â”€ middleware.ts                           # Middleware principal
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ ai-crawler-logger.ts               # Sistema de logging
â”‚   â”œâ”€â”€ ai-crawler-monitor.ts              # Monitoreo y health checks
â”‚   â””â”€â”€ ai-crawler-security.ts             # Seguridad y rate limiting
â”œâ”€â”€ app/api/admin/
â”‚   â”œâ”€â”€ ai-crawler-monitor/route.ts        # API de monitoreo
â”‚   â””â”€â”€ ai-crawler-security/route.ts       # API de seguridad
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ monitor-ai-crawlers.js             # Script de monitoreo CLI
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ ai-crawlers/                       # Logs de crawlers IA
â””â”€â”€ docs/
    â”œâ”€â”€ AI_CRAWLER_CONFIGURATION.md        # DocumentaciÃ³n tÃ©cnica
    â””â”€â”€ OPERATIONAL_PROCEDURES.md          # Procedimientos operativos
```

---

## ğŸ¤ ContribuciÃ³n

### Agregar Nuevo Crawler

1. Actualizar `lib/ai-crawler-security.ts`:
```typescript
const DEFAULT_SECURITY_CONFIG = {
  crawlerRateLimits: {
    'NewBot': {
      requestsPerMinute: 30,
      requestsPerHour: 1000,
      requestsPerDay: 10000,
      burstLimit: 10
    }
  }
}
```

2. Actualizar `app/robots.ts`:
```typescript
const aiCrawlers = ['GPTBot', 'Google-Extended', 'ClaudeBot', 'NewBot']
```

3. Probar configuraciÃ³n:
```bash
curl -H "User-Agent: NewBot/1.0" https://evolve2digital.com/es
```

### Modificar Rate Limits

```bash
curl -X POST "/api/admin/ai-crawler-security?action=rate-limits" \
  -H "Content-Type: application/json" \
  -d '{
    "crawlerType": "GPTBot",
    "limits": {
      "requestsPerMinute": 50,
      "requestsPerHour": 2000
    }
  }'
```

---

## ğŸ“ Soporte

### Contactos

- **Desarrollo**: Equipo de desarrollo
- **Operaciones**: DevOps team
- **Seguridad**: Security team

### EscalaciÃ³n

| Severidad | Tiempo | Criterios |
|-----------|--------|-----------|
| **P1** | 15 min | Sitio inaccesible, brecha de seguridad |
| **P2** | 2 horas | Rate limiting excesivo, health checks fallando |
| **P3** | 24 horas | Optimizaciones, ajustes |
| **P4** | 72 horas | Mejoras, documentaciÃ³n |

---

## ğŸ“Š Estado del Sistema

### Ãšltima VerificaciÃ³n
- **Fecha**: Enero 2024
- **Estado**: âœ… Operacional
- **VersiÃ³n**: 1.0.0
- **Crawlers Activos**: 5
- **Success Rate**: >99%

### PrÃ³ximas Mejoras
- ğŸ”„ Dashboard web de administraciÃ³n
- ğŸ”„ IntegraciÃ³n con sistemas de alertas externos
- ğŸ”„ AnÃ¡lisis de contenido indexado
- ğŸ”„ MÃ©tricas de SEO para IA

---

**ğŸš€ Sistema listo para producciÃ³n**  
**ğŸ“ˆ Monitoreo activo 24/7**  
**ğŸ”’ Seguridad robusta implementada**