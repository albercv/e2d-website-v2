# ü§ñ Sistema de Rastreo IA - evolve2digital.com

## üìã Resumen

Sistema completo de configuraci√≥n, monitoreo y seguridad para crawlers de IA (GPTBot, Google-Extended, ClaudeBot, etc.) implementado en Next.js con TypeScript.

### ‚ú® Caracter√≠sticas Principales

- üéØ **Acceso Estrat√©gico**: Permite acceso controlado a crawlers IA leg√≠timos
- üìä **Monitoreo Completo**: Logging detallado y m√©tricas en tiempo real
- üîí **Seguridad Robusta**: Rate limiting, detecci√≥n de anomal√≠as y protecci√≥n contra abusos
- üè• **Health Checks**: Verificaci√≥n autom√°tica de accesibilidad
- üö® **Alertas**: Sistema de notificaciones para incidentes
- üìà **APIs de Gesti√≥n**: Endpoints para administraci√≥n y configuraci√≥n

---

## üöÄ Inicio R√°pido

### Verificar Estado del Sistema

```bash
# Comprobar que todo funciona
node scripts/monitor-ai-crawlers.js stats

# Probar acceso de GPTBot
curl -H "User-Agent: GPTBot/1.0" https://evolve2digital.com/es

# Verificar logs
tail -f logs/ai-crawlers/$(date +%Y-%m-%d).log
```

### Comandos Esenciales

```bash
# Monitoreo autom√°tico
node scripts/monitor-ai-crawlers.js monitor

# Estad√≠sticas de seguridad
curl -X GET "http://localhost:3000/api/admin/ai-crawler-security?action=stats"

# Health check espec√≠fico
curl -X GET "http://localhost:3000/api/admin/ai-crawler-monitor?action=health-check&crawler=GPTBot"
```

---

## üèóÔ∏è Arquitectura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   robots.txt    ‚îÇ    ‚îÇ   Middleware     ‚îÇ    ‚îÇ   AI Logger     ‚îÇ
‚îÇ   (Permisos)    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   (Seguridad)    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   (Registro)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Monitor API   ‚îÇ    ‚îÇ   Security API   ‚îÇ    ‚îÇ   Health Check  ‚îÇ
‚îÇ   (Estad√≠sticas)‚îÇ    ‚îÇ   (Configuraci√≥n)‚îÇ    ‚îÇ   (Verificaci√≥n)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Componentes Clave

| Componente | Archivo | Descripci√≥n |
|------------|---------|-------------|
| **Robots Config** | `app/robots.ts` | Configuraci√≥n de permisos para crawlers |
| **Middleware** | `middleware.ts` | Interceptor de requests con seguridad |
| **AI Logger** | `lib/ai-crawler-logger.ts` | Sistema de logging especializado |
| **Monitor** | `lib/ai-crawler-monitor.ts` | Health checks y alertas |
| **Security** | `lib/ai-crawler-security.ts` | Rate limiting y protecci√≥n |
| **Monitor Script** | `scripts/monitor-ai-crawlers.js` | Herramienta de monitoreo CLI |

---

## üîß Configuraci√≥n

### Crawlers Soportados

| Crawler | User-Agent | Rate Limit | Estado |
|---------|------------|------------|--------|
| **GPTBot** | `GPTBot/1.0` | 30 req/min | ‚úÖ Activo |
| **Google-Extended** | `Google-Extended/1.0` | 60 req/min | ‚úÖ Activo |
| **ClaudeBot** | `ClaudeBot/1.0` | 30 req/min | ‚úÖ Activo |
| **ChatGPT-User** | `ChatGPT-User/1.0` | 20 req/min | ‚úÖ Activo |
| **Bingbot** | `Bingbot/2.0` | 40 req/min | ‚úÖ Activo |

### Rutas Permitidas

```typescript
const allowedPaths = [
  '/',              // P√°gina principal
  '/es/',           // Espa√±ol
  '/en/',           // Ingl√©s
  '/es/blog/',      // Blog espa√±ol
  '/en/blog/',      // Blog ingl√©s
  '/es/docs/',      // Documentaci√≥n espa√±ol
  '/en/docs/',      // Documentaci√≥n ingl√©s
  '/sitemap.xml',   // Sitemap
  '/rss.xml'        // RSS Feed
]
```

### Rutas Bloqueadas

```typescript
const disallowedPaths = [
  '/api/',          // APIs internas
  '/admin/',        // Panel de administraci√≥n
  '/_next/',        // Archivos de Next.js
  '/private/',      // Contenido privado
  '/*.json$'        // Archivos JSON
]
```

---

## üìä APIs de Administraci√≥n

### Monitor API

```bash
# Estad√≠sticas generales
GET /api/admin/ai-crawler-monitor?action=stats

# Health check espec√≠fico
GET /api/admin/ai-crawler-monitor?action=health-check&crawler=GPTBot&url=https://evolve2digital.com/es

# Configuraci√≥n actual
GET /api/admin/ai-crawler-monitor?action=config

# Ejecutar monitoreo completo
POST /api/admin/ai-crawler-monitor?action=run-cycle

# Probar crawler espec√≠fico
POST /api/admin/ai-crawler-monitor?action=test-crawler
```

### Security API

```bash
# Estad√≠sticas de seguridad
GET /api/admin/ai-crawler-security?action=stats

# Configuraci√≥n de seguridad
GET /api/admin/ai-crawler-security?action=config

# Activar modo emergencia
POST /api/admin/ai-crawler-security?action=emergency-mode

# Gestionar blacklist
POST /api/admin/ai-crawler-security?action=blacklist

# Actualizar rate limits
POST /api/admin/ai-crawler-security?action=rate-limits

# Limpiar datos antiguos
DELETE /api/admin/ai-crawler-security?action=cleanup
```

---

## üõ†Ô∏è Instalaci√≥n y Setup

### Requisitos

- Node.js 18+
- Next.js 14+
- TypeScript
- Permisos de escritura en `/logs/`

### Instalaci√≥n

```bash
# 1. Los archivos ya est√°n incluidos en el proyecto
# 2. Crear directorio de logs
mkdir -p logs/ai-crawlers

# 3. Hacer ejecutable el script de monitoreo
chmod +x scripts/monitor-ai-crawlers.js

# 4. Probar configuraci√≥n
npm run dev
node scripts/monitor-ai-crawlers.js test
```

### Configuraci√≥n de Producci√≥n

```bash
# 1. Configurar cron job para monitoreo autom√°tico
crontab -e
# Agregar: 0,30 * * * * /usr/bin/node /path/to/monitor-ai-crawlers.js monitor

# 2. Configurar rotaci√≥n de logs (opcional)
# Agregar a logrotate.d/ai-crawlers:
# /path/to/logs/ai-crawlers/*.log {
#   daily
#   rotate 30
#   compress
#   delaycompress
#   missingok
#   notifempty
# }

# 3. Verificar permisos
chown -R www-data:www-data logs/ai-crawlers/
chmod 755 logs/ai-crawlers/
```

---

## üìà Monitoreo y M√©tricas

### M√©tricas Clave

- **Success Rate**: >95% esperado
- **Response Time**: <2000ms promedio
- **Rate Limit Violations**: <10/d√≠a
- **Security Violations**: 0 cr√≠ticas
- **Active Crawlers**: 3-5 simult√°neos

### Comandos de Monitoreo

```bash
# Dashboard r√°pido
node scripts/monitor-ai-crawlers.js stats

# Logs en tiempo real
tail -f logs/ai-crawlers/$(date +%Y-%m-%d).log

# Estad√≠sticas de seguridad
curl -s -X GET "http://localhost:3000/api/admin/ai-crawler-security?action=stats" | jq

# Health check manual
curl -H "User-Agent: GPTBot/1.0" -v https://evolve2digital.com/es
```

### Alertas Autom√°ticas

El sistema genera alertas cuando:
- Success rate <95%
- Response time >5000ms
- >10 errores por hora
- Violaciones de seguridad cr√≠ticas
- Health checks fallan consecutivamente

---

## üö® Troubleshooting

### Problemas Comunes

#### 1. Crawler No Puede Acceder (403/404)

```bash
# Verificar robots.txt
curl https://evolve2digital.com/robots.txt

# Comprobar blacklist
curl -s -X GET "/api/admin/ai-crawler-security?action=config" | jq '.data.ipBlacklist'

# Probar acceso manual
curl -H "User-Agent: GPTBot/1.0" -v https://evolve2digital.com/es
```

#### 2. Rate Limiting Excesivo (429)

```bash
# Revisar l√≠mites actuales
curl -s -X GET "/api/admin/ai-crawler-security?action=config" | jq '.data.crawlerRateLimits'

# Aumentar l√≠mites temporalmente
curl -X POST "/api/admin/ai-crawler-security?action=rate-limits" \
  -H "Content-Type: application/json" \
  -d '{"crawlerType": "GPTBot", "limits": {"requestsPerMinute": 100}}'
```

#### 3. Logs No Se Generan

```bash
# Verificar permisos
ls -la logs/ai-crawlers/

# Comprobar middleware
grep "AI-CRAWLER" logs/application.log

# Probar logging manual
curl -H "User-Agent: GPTBot/1.0" https://evolve2digital.com/es
tail -1 logs/ai-crawlers/$(date +%Y-%m-%d).log
```

### Diagn√≥stico R√°pido

```bash
# Script de diagn√≥stico completo
cat > scripts/diagnose.sh << 'EOF'
#!/bin/bash
echo "=== DIAGN√ìSTICO SISTEMA AI CRAWLER ==="
echo "1. Estado del servidor:"
curl -I https://evolve2digital.com/ 2>/dev/null | head -1

echo "2. Robots.txt:"
curl -s https://evolve2digital.com/robots.txt | grep -E "(GPTBot|Allow|Disallow)" | head -5

echo "3. Middleware activo:"
curl -H "User-Agent: GPTBot/1.0" -I https://evolve2digital.com/es 2>/dev/null | head -1

echo "4. Logs del d√≠a:"
ls -la logs/ai-crawlers/$(date +%Y-%m-%d).log 2>/dev/null || echo "‚ùå No hay logs hoy"

echo "5. API de monitoreo:"
curl -s -X GET "http://localhost:3000/api/admin/ai-crawler-monitor?action=stats" >/dev/null 2>&1 && echo "‚úÖ API responde" || echo "‚ùå API no responde"

echo "=== FIN DIAGN√ìSTICO ==="
EOF

chmod +x scripts/diagnose.sh
./scripts/diagnose.sh
```

---

## üîí Seguridad

### Medidas Implementadas

- **Rate Limiting**: L√≠mites por crawler y por IP
- **Blacklist/Whitelist**: Control de IPs
- **Detecci√≥n de Anomal√≠as**: Patrones sospechosos
- **Modo Emergencia**: Bloqueo temporal en caso de ataques
- **Logging Seguro**: Rotaci√≥n autom√°tica y cumplimiento GDPR

### Configuraci√≥n de Seguridad

```typescript
const securityConfig = {
  rateLimits: {
    'GPTBot': { requestsPerMinute: 30, burstLimit: 10 },
    'Google-Extended': { requestsPerMinute: 60, burstLimit: 15 }
  },
  anomalyPatterns: ['/admin', '/.env', '/wp-admin'],
  emergencyMode: { enabled: false, maxRequestsPerMinute: 5 }
}
```

### Respuesta a Incidentes

```bash
# Activar modo emergencia
curl -X POST "/api/admin/ai-crawler-security?action=emergency-mode" \
  -d '{"enabled": true, "maxRequestsPerMinute": 5}'

# Bloquear IP problem√°tica
curl -X POST "/api/admin/ai-crawler-security?action=blacklist" \
  -d '{"ip": "MALICIOUS_IP", "operation": "add"}'

# Limpiar violaciones
curl -X DELETE "/api/admin/ai-crawler-security?action=cleanup"
```

---

## üìö Documentaci√≥n Adicional

### Archivos de Documentaci√≥n

- [`docs/AI_CRAWLER_CONFIGURATION.md`](docs/AI_CRAWLER_CONFIGURATION.md) - Configuraci√≥n t√©cnica completa
- [`docs/OPERATIONAL_PROCEDURES.md`](docs/OPERATIONAL_PROCEDURES.md) - Procedimientos operativos diarios
- Este archivo - Gu√≠a de inicio r√°pido

### Referencias Externas

- [OpenAI GPTBot Documentation](https://platform.openai.com/docs/gptbot)
- [Google Extended Crawler](https://developers.google.com/search/docs/crawling-indexing/overview-google-crawlers)
- [Anthropic Claude Bot](https://docs.anthropic.com/claude/docs)

### Estructura de Archivos

```
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îî‚îÄ‚îÄ robots.ts                           # Configuraci√≥n robots.txt
‚îú‚îÄ‚îÄ middleware.ts                           # Middleware principal
‚îú‚îÄ‚îÄ lib/
‚îÇ   ‚îú‚îÄ‚îÄ ai-crawler-logger.ts               # Sistema de logging
‚îÇ   ‚îú‚îÄ‚îÄ ai-crawler-monitor.ts              # Monitoreo y health checks
‚îÇ   ‚îî‚îÄ‚îÄ ai-crawler-security.ts             # Seguridad y rate limiting
‚îú‚îÄ‚îÄ app/api/admin/
‚îÇ   ‚îú‚îÄ‚îÄ ai-crawler-monitor/route.ts        # API de monitoreo
‚îÇ   ‚îî‚îÄ‚îÄ ai-crawler-security/route.ts       # API de seguridad
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ monitor-ai-crawlers.js             # Script de monitoreo CLI
‚îú‚îÄ‚îÄ logs/
‚îÇ   ‚îî‚îÄ‚îÄ ai-crawlers/                       # Logs de crawlers IA
‚îî‚îÄ‚îÄ docs/
    ‚îú‚îÄ‚îÄ AI_CRAWLER_CONFIGURATION.md        # Documentaci√≥n t√©cnica
    ‚îî‚îÄ‚îÄ OPERATIONAL_PROCEDURES.md          # Procedimientos operativos
```

---

## ü§ù Contribuci√≥n

### Agregar Nuevo Crawler

1. Actualizar `lib/ai-crawler-security.ts`:
```typescript
const DEFAULT_SECURITY_CONFIG = {
  crawlerRateLimits: {
    'NewBot': {
      requestsPerMinute: 30,
      requestsPerHour: 1000,
      requestsPerDay: 10000,
      burstLimit: 10
    }
  }
}
```

2. Actualizar `app/robots.ts`:
```typescript
const aiCrawlers = ['GPTBot', 'Google-Extended', 'ClaudeBot', 'NewBot']
```

3. Probar configuraci√≥n:
```bash
curl -H "User-Agent: NewBot/1.0" https://evolve2digital.com/es
```

### Modificar Rate Limits

```bash
curl -X POST "/api/admin/ai-crawler-security?action=rate-limits" \
  -H "Content-Type: application/json" \
  -d '{
    "crawlerType": "GPTBot",
    "limits": {
      "requestsPerMinute": 50,
      "requestsPerHour": 2000
    }
  }'
```

---

## üìû Soporte

### Contactos

- **Desarrollo**: Equipo de desarrollo
- **Operaciones**: DevOps team
- **Seguridad**: Security team

### Escalaci√≥n

| Severidad | Tiempo | Criterios |
|-----------|--------|-----------|
| **P1** | 15 min | Sitio inaccesible, brecha de seguridad |
| **P2** | 2 horas | Rate limiting excesivo, health checks fallando |
| **P3** | 24 horas | Optimizaciones, ajustes |
| **P4** | 72 horas | Mejoras, documentaci√≥n |

---

## üìä Estado del Sistema

### √öltima Verificaci√≥n
- **Fecha**: Enero 2024
- **Estado**: ‚úÖ Operacional
- **Versi√≥n**: 1.0.0
- **Crawlers Activos**: 5
- **Success Rate**: >99%

### Pr√≥ximas Mejoras
- üîÑ Dashboard web de administraci√≥n
- üîÑ Integraci√≥n con sistemas de alertas externos
- üîÑ An√°lisis de contenido indexado
- üîÑ M√©tricas de SEO para IA

---

**üöÄ Sistema listo para producci√≥n**  
**üìà Monitoreo activo 24/7**  
**üîí Seguridad robusta implementada**