# Procedimientos Operativos - Sistema de Rastreo IA

## üéØ Gu√≠a de Operaciones Diarias

### ‚úÖ Checklist Diario (5 minutos)

```bash
# 1. Verificar estado general del sistema
node scripts/monitor-ai-crawlers.js stats

# 2. Revisar logs del d√≠a actual
tail -n 50 logs/ai-crawlers/$(date +%Y-%m-%d).log

# 3. Comprobar alertas de seguridad
curl -s -X GET "http://localhost:3000/api/admin/ai-crawler-security?action=stats" | jq '.data.recentViolations'

# 4. Verificar health checks
curl -s -X GET "http://localhost:3000/api/admin/ai-crawler-monitor?action=stats" | jq '.data.healthStatus'
```

### üìä Dashboard de M√©tricas Clave

Revisar diariamente estos indicadores:

| M√©trica | Valor Esperado | Acci√≥n si Fuera de Rango |
|---------|----------------|--------------------------|
| **Success Rate** | >95% | Investigar errores 4xx/5xx |
| **Avg Response Time** | <2000ms | Optimizar rendimiento |
| **Rate Limit Violations** | <10/d√≠a | Ajustar l√≠mites |
| **Security Violations** | 0 | Revisar patrones sospechosos |
| **Active Crawlers** | 3-5 | Verificar configuraci√≥n |

---

## üîß Procedimientos de Mantenimiento

### Semanal (15 minutos)

#### 1. Limpieza de Logs

```bash
# Ejecutar limpieza autom√°tica (mantiene √∫ltimos 30 d√≠as)
curl -X DELETE "http://localhost:3000/api/admin/ai-crawler-security?action=cleanup"

# Verificar espacio en disco
du -sh logs/ai-crawlers/
```

#### 2. An√°lisis de Tendencias

```bash
# Generar reporte semanal
node scripts/monitor-ai-crawlers.js stats > reports/weekly-$(date +%Y-%m-%d).json

# Revisar crawlers m√°s activos
grep -c "GPTBot" logs/ai-crawlers/*.log | sort -t: -k2 -nr | head -5
```

#### 3. Optimizaci√≥n de Rate Limits

```bash
# Revisar violaciones por crawler
curl -s -X GET "http://localhost:3000/api/admin/ai-crawler-security?action=stats" | \
  jq '.data.crawlerMetrics | to_entries[] | select(.value.rateLimitViolations > 0)'

# Ajustar l√≠mites si es necesario (ejemplo para GPTBot)
curl -X POST "http://localhost:3000/api/admin/ai-crawler-security?action=rate-limits" \
  -H "Content-Type: application/json" \
  -d '{"crawlerType": "GPTBot", "limits": {"requestsPerMinute": 40}}'
```

### Mensual (30 minutos)

#### 1. Auditor√≠a Completa

```bash
# Generar reporte mensual completo
{
  echo "=== REPORTE MENSUAL - $(date) ==="
  echo ""
  echo "1. ESTAD√çSTICAS GENERALES:"
  node scripts/monitor-ai-crawlers.js stats
  echo ""
  echo "2. SEGURIDAD:"
  curl -s -X GET "http://localhost:3000/api/admin/ai-crawler-security?action=stats"
  echo ""
  echo "3. HEALTH CHECKS:"
  curl -s -X GET "http://localhost:3000/api/admin/ai-crawler-monitor?action=stats"
} > reports/monthly-$(date +%Y-%m).txt
```

#### 2. Revisi√≥n de Configuraci√≥n

```bash
# Backup de configuraci√≥n actual
cp middleware.ts backups/middleware-$(date +%Y-%m-%d).ts
cp lib/ai-crawler-security.ts backups/ai-crawler-security-$(date +%Y-%m-%d).ts

# Verificar configuraci√≥n de robots.txt
curl https://evolve2digital.com/robots.txt > backups/robots-$(date +%Y-%m-%d).txt
```

#### 3. Actualizaci√≥n de Blacklist

```bash
# Revisar IPs con m√∫ltiples violaciones
curl -s -X GET "http://localhost:3000/api/admin/ai-crawler-security?action=stats" | \
  jq '.data.ipViolations | to_entries[] | select(.value > 10)'

# Agregar IPs problem√°ticas a blacklist (ejemplo)
# curl -X POST "http://localhost:3000/api/admin/ai-crawler-security?action=blacklist" \
#   -H "Content-Type: application/json" \
#   -d '{"ip": "PROBLEMATIC_IP", "operation": "add"}'
```

---

## üö® Procedimientos de Emergencia

### Escenario 1: Tr√°fico Excesivo de Crawlers

**S√≠ntomas**: 
- CPU/memoria alta
- Tiempos de respuesta >5s
- M√∫ltiples 429 errors

**Respuesta Inmediata** (2 minutos):

```bash
# 1. Activar modo emergencia
curl -X POST "http://localhost:3000/api/admin/ai-crawler-security?action=emergency-mode" \
  -H "Content-Type: application/json" \
  -d '{
    "enabled": true,
    "allowedCrawlers": ["GPTBot"],
    "maxRequestsPerMinute": 5,
    "reason": "High traffic emergency"
  }'

# 2. Verificar activaci√≥n
curl -s -X GET "http://localhost:3000/api/admin/ai-crawler-security?action=config" | \
  jq '.data.emergencyMode'

# 3. Monitorear mejora
watch -n 30 'curl -s -X GET "http://localhost:3000/api/admin/ai-crawler-monitor?action=stats" | jq ".data.responseTime"'
```

**Investigaci√≥n** (10 minutos):

```bash
# Identificar crawler problem√°tico
tail -n 100 logs/ai-crawlers/$(date +%Y-%m-%d).log | \
  grep -E "(429|503)" | \
  cut -d'"' -f8 | sort | uniq -c | sort -nr

# Revisar patrones de IP
tail -n 100 logs/ai-crawlers/$(date +%Y-%m-%d).log | \
  grep -E "(429|503)" | \
  cut -d'"' -f6 | sort | uniq -c | sort -nr
```

**Resoluci√≥n**:

```bash
# Desactivar modo emergencia cuando se estabilice
curl -X POST "http://localhost:3000/api/admin/ai-crawler-security?action=emergency-mode" \
  -H "Content-Type: application/json" \
  -d '{"enabled": false}'
```

### Escenario 2: Crawler Malicioso

**S√≠ntomas**:
- Requests a rutas sensibles (/admin, /.env)
- Velocidad excesiva (>100 req/min)
- User-Agent sospechoso

**Respuesta Inmediata**:

```bash
# 1. Identificar IP problem√°tica
tail -n 200 logs/ai-crawlers/$(date +%Y-%m-%d).log | \
  grep -E "(admin|\.env|wp-admin)" | \
  cut -d'"' -f6 | sort | uniq -c | sort -nr

# 2. Bloquear IP inmediatamente
MALICIOUS_IP="192.168.1.100"  # Reemplazar con IP real
curl -X POST "http://localhost:3000/api/admin/ai-crawler-security?action=blacklist" \
  -H "Content-Type: application/json" \
  -d "{\"ip\": \"$MALICIOUS_IP\", \"operation\": \"add\"}"

# 3. Verificar bloqueo
curl -s -X GET "http://localhost:3000/api/admin/ai-crawler-security?action=config" | \
  jq ".data.ipBlacklist[] | select(. == \"$MALICIOUS_IP\")"
```

### Escenario 3: Health Checks Fallando

**S√≠ntomas**:
- Script de monitoreo reporta errores
- Crawlers leg√≠timos no pueden acceder
- Success rate <90%

**Diagn√≥stico**:

```bash
# 1. Probar acceso manual
curl -H "User-Agent: GPTBot/1.0" -v https://evolve2digital.com/es

# 2. Verificar robots.txt
curl -v https://evolve2digital.com/robots.txt

# 3. Comprobar middleware
grep -n "AI-CRAWLER" logs/application.log | tail -10

# 4. Revisar configuraci√≥n de seguridad
curl -s -X GET "http://localhost:3000/api/admin/ai-crawler-security?action=config" | \
  jq '.data.emergencyMode, .data.ipBlacklist'
```

**Resoluci√≥n**:

```bash
# Si hay bloqueos incorrectos, limpiar blacklist
curl -X DELETE "http://localhost:3000/api/admin/ai-crawler-security?action=cleanup"

# Reiniciar configuraci√≥n de seguridad si es necesario
curl -X POST "http://localhost:3000/api/admin/ai-crawler-security?action=reset-config"
```

---

## üìã Checklists de Verificaci√≥n

### Pre-Deployment

```bash
# ‚úÖ Verificar configuraci√≥n de robots.txt
curl https://evolve2digital.com/robots.txt | grep -E "(GPTBot|Google-Extended|ClaudeBot)"

# ‚úÖ Probar middleware en desarrollo
npm run dev
curl -H "User-Agent: GPTBot/1.0" http://localhost:3000/es

# ‚úÖ Verificar APIs de administraci√≥n
curl -X GET "http://localhost:3000/api/admin/ai-crawler-monitor?action=config"
curl -X GET "http://localhost:3000/api/admin/ai-crawler-security?action=config"

# ‚úÖ Probar script de monitoreo
node scripts/monitor-ai-crawlers.js test

# ‚úÖ Verificar permisos de logs
ls -la logs/ai-crawlers/
```

### Post-Deployment

```bash
# ‚úÖ Verificar acceso de crawlers en producci√≥n
curl -H "User-Agent: GPTBot/1.0" https://evolve2digital.com/es
curl -H "User-Agent: Google-Extended/1.0" https://evolve2digital.com/en

# ‚úÖ Comprobar logging activo
tail -f logs/ai-crawlers/$(date +%Y-%m-%d).log &
curl -H "User-Agent: GPTBot/1.0" https://evolve2digital.com/es/blog
# Verificar que aparece el log

# ‚úÖ Probar rate limiting
for i in {1..35}; do
  curl -H "User-Agent: GPTBot/1.0" https://evolve2digital.com/es &
done
# Verificar que algunos requests reciben 429

# ‚úÖ Configurar cron job
crontab -l | grep monitor-ai-crawlers
```

### Troubleshooting R√°pido

```bash
# üîç Verificar si el sistema est√° funcionando
echo "=== DIAGN√ìSTICO R√ÅPIDO ==="

# 1. Estado del servidor
curl -I https://evolve2digital.com/

# 2. Robots.txt accesible
curl -s https://evolve2digital.com/robots.txt | head -5

# 3. Middleware activo
curl -H "User-Agent: GPTBot/1.0" -I https://evolve2digital.com/es

# 4. Logs gener√°ndose
ls -la logs/ai-crawlers/$(date +%Y-%m-%d).log 2>/dev/null || echo "‚ùå No hay logs hoy"

# 5. APIs respondiendo
curl -s -X GET "http://localhost:3000/api/admin/ai-crawler-monitor?action=stats" | \
  jq '.success' 2>/dev/null || echo "‚ùå API no responde"

echo "=== FIN DIAGN√ìSTICO ==="
```

---

## üìä M√©tricas y KPIs

### Indicadores Clave de Rendimiento

| KPI | Objetivo | Medici√≥n | Frecuencia |
|-----|----------|----------|------------|
| **Disponibilidad para IA** | >99% | Health checks exitosos | Continua |
| **Tiempo de Respuesta** | <2s | Promedio de response time | Diaria |
| **Cobertura de Contenido** | >95% | URLs indexables accesibles | Semanal |
| **Seguridad** | 0 violaciones cr√≠ticas | Intentos de acceso malicioso | Diaria |
| **Eficiencia de Rate Limiting** | <5% de 429 errors | Requests bloqueados vs totales | Diaria |

### Reportes Autom√°ticos

```bash
# Configurar reporte diario por email (opcional)
# Agregar a crontab:
# 0 9 * * * /path/to/daily-report.sh | mail -s "AI Crawler Daily Report" admin@evolve2digital.com

# Script de reporte diario
cat > scripts/daily-report.sh << 'EOF'
#!/bin/bash
echo "=== REPORTE DIARIO AI CRAWLERS - $(date) ==="
echo ""
echo "üìä ESTAD√çSTICAS:"
node scripts/monitor-ai-crawlers.js stats | head -20
echo ""
echo "üîí SEGURIDAD:"
curl -s -X GET "http://localhost:3000/api/admin/ai-crawler-security?action=stats" | jq '.data.todayViolations'
echo ""
echo "üè• HEALTH STATUS:"
curl -s -X GET "http://localhost:3000/api/admin/ai-crawler-monitor?action=stats" | jq '.data.healthStatus'
EOF

chmod +x scripts/daily-report.sh
```

---

## üîÑ Procedimientos de Actualizaci√≥n

### Actualizar Rate Limits

```bash
# Ejemplo: Aumentar l√≠mite para GPTBot
curl -X POST "http://localhost:3000/api/admin/ai-crawler-security?action=rate-limits" \
  -H "Content-Type: application/json" \
  -d '{
    "crawlerType": "GPTBot",
    "limits": {
      "requestsPerMinute": 50,
      "requestsPerHour": 2000,
      "requestsPerDay": 20000,
      "burstLimit": 15
    }
  }'
```

### Agregar Nuevo Crawler

1. **Actualizar configuraci√≥n en c√≥digo**:
```typescript
// En lib/ai-crawler-security.ts
const DEFAULT_SECURITY_CONFIG = {
  crawlerRateLimits: {
    // ... existentes
    'NewBot': {
      requestsPerMinute: 30,
      requestsPerHour: 1000,
      requestsPerDay: 10000,
      burstLimit: 10
    }
  }
}
```

2. **Actualizar robots.txt**:
```typescript
// En app/robots.ts
const aiCrawlers = [
  'GPTBot',
  'Google-Extended',
  'ClaudeBot',
  'ChatGPT-User',
  'Bingbot',
  'NewBot'  // Agregar aqu√≠
]
```

3. **Probar configuraci√≥n**:
```bash
curl -H "User-Agent: NewBot/1.0" https://evolve2digital.com/es
```

### Modificar Rutas Permitidas

```typescript
// En app/robots.ts - Agregar nueva ruta
const allowedPaths = [
  '/',
  '/es/',
  '/en/',
  '/es/blog/',
  '/en/blog/',
  '/es/docs/',
  '/en/docs/',
  '/nueva-seccion/',  // Nueva ruta
  '/sitemap.xml',
  '/rss.xml'
]
```

---

## üìû Contactos y Escalaci√≥n

### Niveles de Soporte

1. **Nivel 1 - Operaciones Diarias**: Desarrollador/DevOps
2. **Nivel 2 - Incidentes Cr√≠ticos**: Tech Lead
3. **Nivel 3 - Emergencias de Seguridad**: CTO/Security Team

### Procedimiento de Escalaci√≥n

| Severidad | Tiempo de Respuesta | Criterios |
|-----------|-------------------|-----------|
| **P1 - Cr√≠tico** | 15 minutos | Sitio inaccesible para crawlers, brecha de seguridad |
| **P2 - Alto** | 2 horas | Rate limiting excesivo, health checks fallando |
| **P3 - Medio** | 24 horas | Optimizaciones, ajustes de configuraci√≥n |
| **P4 - Bajo** | 72 horas | Mejoras, reportes, documentaci√≥n |

---

**√öltima actualizaci√≥n**: Enero 2024  
**Versi√≥n**: 1.0.0  
**Pr√≥xima revisi√≥n**: Febrero 2024